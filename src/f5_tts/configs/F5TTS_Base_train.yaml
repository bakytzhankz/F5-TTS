run:
  dir: ckpts/${model.name}_${model.mel_spec.mel_spec_type}_${model.tokenizer}_${datasets.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

datasets:
  name: f5-tts-kazakh # Your custom dataset
  batch_size_per_gpu: 38400 # 8 GPUs, 8 * 38400 = 307200 frames in total
  batch_size_type: frame # "frame" ensures a frame-wise batch size
  max_samples: 64 # Set to 64 for larger base models
  num_workers: 16 # Adequate number of workers for large datasets

optim:
  epochs: 15 # Number of epochs to train
  learning_rate: 7.5e-5 # Optimized learning rate
  num_warmup_updates: 20000 # Warm-up steps for smooth convergence
  grad_accumulation_steps: 1 # For better gradient accumulation per step
  max_grad_norm: 1.0 # To prevent gradient explosion
  bnb_optimizer: False # Using 8-bit AdamW optimizer is optional, False for now for better control

model:
  name: F5TTS_Base # Base model for balanced performance
  tokenizer: custom # Tokenizer for handling Kazakh language
  tokenizer_path: data/f5-tts-kazakh/dataset/M1/output/vocab.txt # Use default tokenizer path unless using custom
  arch:
    dim: 1024 # Model dimensionality for sufficient capacity
    depth: 22 # Depth of the Transformer model
    heads: 16 # Number of attention heads
    ff_mult: 2 # Feed-forward network multiplier
    text_dim: 512 # Dimensionality of text embedding
    conv_layers: 4 # Convolutional layers for better feature extraction
    checkpoint_activations: False # False to save memory, but may require recomputation
  mel_spec:
    target_sample_rate: 24000 # Target sample rate for the audio
    n_mel_channels: 100 # Number of Mel channels (adjust based on memory and performance tradeoff)
    hop_length: 256 # Hop length for spectrogram computation
    win_length: 1024 # Window length for FFT
    n_fft: 1024 # FFT size
    mel_spec_type: vocos # Use 'vocos' or 'bigvgan' based on your experiment setup
  vocoder:
    is_local: False # Set to False to use an external vocoder
    local_path: None # Specify local vocoder path if needed

ckpts:
  logger: wandb # Use WandB for logging (or switch to tensorboard if preferred)
  save_per_updates: 50000 # Save checkpoint every 50k steps
  last_per_steps: 5000 # Save final checkpoints every 5000 steps
  save_dir: ckpts/${model.name}_${model.mel_spec.mel_spec_type}_${model.tokenizer}_${datasets.name}
